{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying exoplanets with Kepler data\n",
    "\n",
    "## Trevor Santiago\n",
    "\n",
    "#### Can we predict whether a record is an exoplanet or not given certain measurements?\n",
    "\n",
    "- How accurate can we do so?\n",
    "- What measurements are the most important for our prediction?\n",
    "\n",
    "\n",
    "[Data Source](https://www.kaggle.com/nasa/kepler-exoplanet-search-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, \\\n",
    "                            GradientBoostingClassifier, BaggingClassifier, \\\n",
    "                            StackingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cumulative.csv').drop(columns=['rowid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-useful and data leakage columns\n",
    "non_inputs = [\n",
    "    'kepid', 'kepoi_name', 'kepler_name', \n",
    "    'koi_score', 'koi_tce_delivname', 'koi_fpflag_nt', \n",
    "    'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec',\n",
    "    'koi_disposition', 'koi_pdisposition'\n",
    "]\n",
    "non_inputs.extend([col for col in data.columns if '_err' in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up features and label\n",
    "\n",
    "Our target `koi_pdisposition` is a string with values `\"CANDIDATE\"` and `\"FALSE POSITIVE\"`, so we need to encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "y = data.koi_pdisposition.copy()\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-way split data\n",
    "\n",
    "These sets will stay the same throughout to accurately compare models. Model performance on the test set will be checked once at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "For each of our models we will drop the data leakage/unimportant columns (see above), impute any missing values with the median of the respective columns, scale the data, and then fit the model. \n",
    "\n",
    "Since there are no categorical variables and we are interested in the importance of each feature at the end, we will keep this Pipeline mostly the same throughout.\n",
    "\n",
    "Our main evaluation metric will be accuracy, but we will also take a look at log-loss to see the confidence of our models.\n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "\n",
    "We'll start with a very basic model as a baseline to compare improvement of our future models. First we define our numeric and categorical columns for preprocessing steps. Then we define our main preprocessing pipelines that will be used throughout. Unimportant and data-leakage columns are discarded, then numeric and categorical columns are split to their respective sub-pipeline for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'koi_srad', 'koi_period', 'koi_time0bk',\n",
    "    'koi_impact', 'koi_duration', 'koi_depth',\n",
    "    'koi_prad', 'koi_teq', 'koi_insol',\n",
    "    'koi_model_snr', 'koi_steff', 'koi_slogg'\n",
    "]\n",
    "\n",
    "cat_cols = ['koi_kepmag', 'koi_tce_plnt_num']\n",
    "\n",
    "# Need to convert the above to indexes for ColumnTransformer\n",
    "num_cols = list(map(lambda x: int(np.where(data.columns == x)[0][0]), num_cols))\n",
    "cat_cols = list(map(lambda x: int(np.where(data.columns == x)[0][0]), cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prep = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='median')),\n",
    "    ('scale', RobustScaler())\n",
    "])\n",
    "\n",
    "# Bin koi_kepmag\n",
    "binner = ColumnTransformer([('bin', KBinsDiscretizer(), [0])], remainder='passthrough')\n",
    "cat_prep = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "    ('bin', binner)\n",
    "])\n",
    "\n",
    "prepper = ColumnTransformer([\n",
    "    ('numeric', num_prep, num_cols),\n",
    "    ('catergorical', cat_prep, cat_cols)\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 1.65599\n",
      "Accuracy: 80.1%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use prediction probabilities for log-loss\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "\n",
    "# Get predictions from probabilities above\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic model gives decent accuracy, but the log-loss is quite high which can mean it is either too confident in its incorrect predictions or not very confident in its correct predictions. Now we will try to improve both of these metrics\n",
    "\n",
    "## Model improvement\n",
    "\n",
    "First we will try bagging on the `KNeighborsClassifier` from the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "bag = BaggingClassifier(knn,\n",
    "                        n_estimators=20,\n",
    "                        oob_score=True,\n",
    "                        n_jobs=-1)\n",
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', bag)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.93222\n",
      "Accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get about the same level of accuracy, but a good improvement in log-loss. Now let's try a couple other simple yet common models\n",
    "\n",
    "Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.50706\n",
      "Accuracy: 77.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse accuracy but much better log-loss.\n",
    "\n",
    "Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 8.18226\n",
      "Accuracy: 76.3%\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better accuracy than Logistic but horrendous log-loss. Let's try voting with these 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter = VotingClassifier(estimators=[\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('logistic', LogisticRegression(max_iter=500)),\n",
    "    ('dtree', DecisionTreeClassifier())\n",
    "], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.43038\n",
      "Accuracy: 80.8%\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', voter)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Accuracy of KNN with an improvement on Logistic's log-loss. Now let's try some stronger algorithms.\n",
    "\n",
    "### More Robust models\n",
    "\n",
    "We will try 3 ensembling models. To save some work, we'll wrap these into a cross-validation with hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Best estimator:  GradientBoostingClassifier(max_depth=5, n_estimators=200, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', DummyEstimator())\n",
    "])\n",
    "\n",
    "search_space = [\n",
    "    {\n",
    "        'clf': [ExtraTreesClassifier()],\n",
    "        'clf__n_estimators': range(50, 301, 50),\n",
    "        'clf__min_samples_leaf': [1, 2],\n",
    "        'clf__criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    {\n",
    "        'clf': [RandomForestClassifier()],\n",
    "        'clf__n_estimators': range(50, 301, 50),\n",
    "        'clf__min_samples_leaf': [1, 2],\n",
    "        'clf__max_features': ['sqrt', 'log2', 0.5],\n",
    "        'clf__criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    {\n",
    "        'clf': [GradientBoostingClassifier()],\n",
    "        'clf__n_estimators': range(50, 301, 50),\n",
    "        'clf__max_depth': list(range(3, 6))+[None],\n",
    "        'clf__subsample': [0.8, 0.9, 1.0],\n",
    "        'clf__loss': ['deviance', 'exponential']        \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "clf_algos_rand = RandomizedSearchCV(estimator=model, \n",
    "                                    param_distributions=search_space, \n",
    "                                    n_iter=25,\n",
    "                                    cv=5, \n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1,\n",
    "                                    scoring='accuracy')\n",
    "\n",
    "clf_algos_rand.fit(X_train, y_train)\n",
    "print('Best estimator: ', clf_algos_rand.best_estimator_['clf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the best estimator from above and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.36227\n",
      "Accuracy: 85.5%\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = clf_algos_rand.best_estimator_.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Nice jump in accuracy, and still lower log-loss. How about using this as a meta-learner in a stack? In order to make this work we need to define individual pipelines for each of the base estimators. Then we pass those in along with this meta-learner to a `StackingClassifier`. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('kneighbors', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('reg', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "dtree_pipe = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Collect all base learner pipes to send into stack\n",
    "base_estimators = [\n",
    "    ('knn', knn_pipe),\n",
    "    ('lr', lr_pipe),\n",
    "    ('dtree', dtree_pipe)\n",
    "]\n",
    "\n",
    "# Create stack, passing original data on to meta-learner\n",
    "stack = StackingClassifier(estimators=base_estimators,\n",
    "                           final_estimator=clf_algos_rand.best_estimator_,\n",
    "                           passthrough=True,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.31403\n",
      "Accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "stack.fit(X_train, y_train)\n",
    "y_pred_probs = stack.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Fair improvement to both accuracy and log-loss. Finally, we'll give `XGBoost` a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.39328\n",
      "Accuracy: 84.2%\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('preprocess', prepper),\n",
    "    ('clf', XGBClassifier(eval_metric='error', use_label_encoder=False, n_jobs=-1))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_probs = model.predict_proba(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_val, y_pred_probs)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, but not worth it over the stack. We'll determine the stack as our final model.\n",
    "\n",
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine stack for refitting on all training & validation data\n",
    "final_stack = StackingClassifier(estimators=base_estimators,\n",
    "                                 final_estimator=clf_algos_rand.best_estimator_,\n",
    "                                 passthrough=True,\n",
    "                                 n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_val])\n",
    "y = LabelEncoder().fit_transform(y=X.koi_pdisposition.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on all data and eval on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 0.32342\n",
      "Accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "final_stack.fit(X, y)\n",
    "y_pred_probs = final_stack.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "loss = log_loss(y_test, y_pred_probs)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'log loss: {loss:.5f}')\n",
    "print(f'Accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now let's see what features helped us get here.\n",
    "\n",
    "### Check which features were most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(final_stack, X, y, n_repeats=30, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>koi_duration</th>\n",
       "      <td>0.094484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_model_snr</th>\n",
       "      <td>0.073070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_period</th>\n",
       "      <td>0.071881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_prad</th>\n",
       "      <td>0.055226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_slogg</th>\n",
       "      <td>0.038571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_insol</th>\n",
       "      <td>0.032292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_teq</th>\n",
       "      <td>0.031595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_depth</th>\n",
       "      <td>0.021660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_time0bk</th>\n",
       "      <td>0.021595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_impact</th>\n",
       "      <td>0.021126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_steff</th>\n",
       "      <td>0.017529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_srad</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_kepmag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koi_tce_plnt_num</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "koi_duration      0.094484\n",
       "koi_model_snr     0.073070\n",
       "koi_period        0.071881\n",
       "koi_prad          0.055226\n",
       "koi_slogg         0.038571\n",
       "koi_insol         0.032292\n",
       "koi_teq           0.031595\n",
       "koi_depth         0.021660\n",
       "koi_time0bk       0.021595\n",
       "koi_impact        0.021126\n",
       "koi_steff         0.017529\n",
       "koi_srad          0.000000\n",
       "koi_kepmag        0.000000\n",
       "koi_tce_plnt_num  0.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = [\n",
    "    'koi_srad', 'koi_period', 'koi_time0bk',\n",
    "    'koi_impact', 'koi_duration', 'koi_depth',\n",
    "    'koi_prad', 'koi_teq', 'koi_insol',\n",
    "    'koi_model_snr', 'koi_steff', 'koi_slogg'\n",
    "]\n",
    "\n",
    "cat_cols = ['koi_kepmag', 'koi_tce_plnt_num']\n",
    "inputs = num_cols + cat_cols\n",
    "pd.DataFrame(data=r.importances_mean, index=X.columns).loc[inputs,:].sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier(estimators=[('knn',\n",
      "                                Pipeline(steps=[('preprocess',\n",
      "                                                 ColumnTransformer(transformers=[('numeric',\n",
      "                                                                                  Pipeline(steps=[('imp',\n",
      "                                                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                                                  ('scale',\n",
      "                                                                                                   RobustScaler())]),\n",
      "                                                                                  [43,\n",
      "                                                                                   10,\n",
      "                                                                                   13,\n",
      "                                                                                   16,\n",
      "                                                                                   19,\n",
      "                                                                                   22,\n",
      "                                                                                   25,\n",
      "                                                                                   28,\n",
      "                                                                                   31,\n",
      "                                                                                   34,\n",
      "                                                                                   37,\n",
      "                                                                                   40]),\n",
      "                                                                                 ('catergorical',\n",
      "                                                                                  Pipeline(steps=[('imp',\n",
      "                                                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                                                  ('bin',\n",
      "                                                                                                   ColumnTransformer(remainder='...\n",
      "                                                                                                      RobustScaler())]),\n",
      "                                                                                     [43,\n",
      "                                                                                      10,\n",
      "                                                                                      13,\n",
      "                                                                                      16,\n",
      "                                                                                      19,\n",
      "                                                                                      22,\n",
      "                                                                                      25,\n",
      "                                                                                      28,\n",
      "                                                                                      31,\n",
      "                                                                                      34,\n",
      "                                                                                      37,\n",
      "                                                                                      40]),\n",
      "                                                                                    ('catergorical',\n",
      "                                                                                     Pipeline(steps=[('imp',\n",
      "                                                                                                      SimpleImputer(strategy='most_frequent')),\n",
      "                                                                                                     ('bin',\n",
      "                                                                                                      ColumnTransformer(remainder='passthrough',\n",
      "                                                                                                                        transformers=[('bin',\n",
      "                                                                                                                                       KBinsDiscretizer(),\n",
      "                                                                                                                                       [0])]))]),\n",
      "                                                                                     [48,\n",
      "                                                                                      35])])),\n",
      "                                                   ('clf',\n",
      "                                                    GradientBoostingClassifier(max_depth=5,\n",
      "                                                                               n_estimators=200,\n",
      "                                                                               subsample=0.8))]),\n",
      "                   n_jobs=-1, passthrough=True)\n"
     ]
    }
   ],
   "source": [
    "print(final_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
